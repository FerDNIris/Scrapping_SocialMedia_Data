# -*- coding: utf-8 -*-
"""Social_Media_Scrapping_and_sentiments_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r0jXImPDld8HoNc7t_Pa8GtScLCMIF8g

# Twitter Data and Sentiment Analysis
### Iris Startup Lab
The main objective of this notebook is the analysis of tweets and their sentiment

## Installing libraries and loading them
"""

!pip install spacy
!pip install tweepy
!pip install google-api-python-client
!pip install google-auth-oauthlib
!pip install google-auth-httplib2
!pip install vaderSentiment
!pip install SpacyTextBlob

!pip install -q -U google-generativeai

### Instalando el modelo en español
!python -m spacy download es_core_news_lg

from google.colab import userdata
import tweepy
import spacy
import pandas as pd
import nltk
import re
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from spacytextblob.spacytextblob import SpacyTextBlob
from googleapiclient.discovery import build

from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline

twitter_api_key = userdata.get('twitter_api_key')
twitter_api_secret = userdata.get('twitter_secret')
twitter_access_token = userdata.get('twitter_access_token')
twitter_access_token_secret = userdata.get('twitter_access_token_secret')
twitter_bearer_token = userdata.get('twitter_bearer_token')
gemini_api_key = userdata.get('Gemini_secret')

import google.generativeai as genai
import os

genai.configure(api_key=gemini_api_key)

import pprint
for model in genai.list_models():
    pprint.pprint(model)

nlp = spacy.load("es_core_news_lg")

# Inicializar el analizador de VADER
analyzer = SentimentIntensityAnalyzer()

tokenizer = T5Tokenizer.from_pretrained("t5-base")
modelSummary = T5ForConditionalGeneration.from_pretrained("t5-base")

classifier = pipeline('sentiment-analysis',
                      model="nlptown/bert-base-multilingual-uncased-sentiment")

def generate_summary(text, max_length=150):
    input_text = "summarize: " + str(text)
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = modelSummary.generate(input_ids, max_length=max_length, num_beams=4, length_penalty=2.0, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

#### Function to get Tweets
def getCommentsFromTweet(tweet_id, bearer_token, max_results=10):
    client = tweepy.Client(bearer_token=bearer_token)
    listTweets = client.search_recent_tweets(
        query=f"conversation_id:{tweet_id}",
        expansions=["author_id"],
        user_fields=["username"],
        max_results=max_results
    )
    tweets = listTweets.data
    users = {u["id"]: u for u in listTweets.includes["users"]}
    tweets_list = []
    for tweet in tweets:
        tweet_info = {
            'tweet_id': tweet.id,
            'text': tweet.text,
            'author_id': tweet.author_id,
            'username': users[tweet.author_id]["username"] if tweet.author_id in users else None
        }
        tweets_list.append(tweet_info)
    df_tweets = pd.DataFrame(tweets_list)
    return df_tweets

def detectTextPolaritySentiment(text):
    try:
        nlpEval = nlp(text)
        return nlpEval._.blob.polarity
    except:
        return 0.0

def detectTextPolaritySentimentVader(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']

def categorySentiment(sentimentScore):
    if sentimentScore>0:
        return 'Positive'
    elif sentimentScore<0:
        return 'Negative'
    else:
        return 'Neutral'

def categorizedRange(number):
    import re
    number = int(re.sub(r'\D', '', str(number)))
    return ((number -1)/4)*2-1

def bertSpanishClassifier(text):
    import numpy as np
    import re
    mainClassifier = classifier(text)
    resultsClassifier = mainClassifier[0]
    scoreClassifier = resultsClassifier['score']
    if scoreClassifier >= 0.75:
        return categorizedRange((resultsClassifier['label']))
    else:
        return None

def combinedSentimentAnalysis(text):
    """
    Combina tres funciones de análisis de sentimiento con jerarquía.

    Args:
        text (str): El texto para analizar.

    Returns:
        float or str or None: El resultado del análisis de sentimiento, siguiendo la jerarquía.
                              Si todas las funciones devuelven None o un valor no válido, devuelve None.
    """
    text = str(text)
    result1 = bertSpanishClassifier(text)
    if result1 is not None:
        return result1

    result2 = detectTextPolaritySentimentVader(text)
    if result2 is not None:
        return result2

    result3 = detectTextPolaritySentiment(text)
    if result3 is not None:
        return result3

    return None

def combinedSentimentAnalysis(text):
    """
    Combina tres funciones de análisis de sentimiento con jerarquía y maneja errores de BERT.

    Args:
        text (str): El texto para analizar.

    Returns:
        float or str or None: El resultado del análisis de sentimiento, siguiendo la jerarquía.
                                        Si todas las funciones devuelven None o un valor no válido, devuelve None.
    """
    text = str(text)
    try:
        result1 = bertSpanishClassifier(text)
        if result1 is not None:
            return result1
    except Exception as e:
        print(f"Error en bertSpanishClassifier: {e}")
        pass

    result2 = detectTextPolaritySentimentVader(text)
    if result2 is not None:
        return result2

    result3 = detectTextPolaritySentiment(text)
    if result3 is not None:
        return result3

    return None

import time
import os

def processDfLimit(df, columna_texto, funcion_api, column_name):
    """
    Procesa un DataFrame aplicando una función de API a cada fila,
    respetando un límite de frecuencia de 10 solicitudes por minuto.
    Args:
        df (pd.DataFrame): El DataFrame a procesar.
        columna_texto (str): El nombre de la columna con el texto a procesar.
        funcion_api (callable): La función que realiza la llamada a la API.
    Returns:
        pd.DataFrame: El DataFrame con los resultados de la API.
    """
    resultados = []
    solicitudes_minuto = 0
    inicio_minuto = time.time()

    for texto in df[columna_texto]:
        resultado = funcion_api(texto)
        resultados.append(resultado)
        solicitudes_minuto += 1

        if solicitudes_minuto >= 10:
            tiempo_transcurrido = time.time() - inicio_minuto
            if tiempo_transcurrido < 60:
                time.sleep(60 - tiempo_transcurrido)
            solicitudes_minuto = 0
            inicio_minuto = time.time()

    df[column_name] = resultados
    return df





'''
# Cargar el Dataframe.

df = pd.read_csv("ejemplo.csv")

# Usar la función con control de frecuencia

df_con_resumenes = procesar_dataframe_con_limite(df, 'texto', generar_resumen)
print(df_con_resumenes)

df_con_sentimientos = procesar_dataframe_con_limite(df, 'texto', analizar_sentimiento)
print(df_con_sentimientos)
'''

def getSummaryIA(text):
    model = genai.GenerativeModel('models/gemini-2.0-flash')
    answer = model.generate_content(f"Resumir el siguiente texto: {text}")
    return answer.text

def getSentimentIA(text):
    #model = genai.GenerativeModel('models/gemini-2.0-flash')
    model = genai.GenerativeModel('models/gemma-3-27b-it')
    answer = model.generate_content(f"Analizar el sentimiento del siguiente texto y proporcionar un resultado positivo, negativo o neutro: {text}")
    return answer.text

import re

def anonimizar_datos(texto):
    """
    Anomiza números de teléfono de 10 dígitos y correos electrónicos en un texto.

    Args:
        texto (str): El texto para anomizar.

    Returns:
        str: El texto anomizado.
    """
    texto = str(texto)
    texto_anonimizado = re.sub(r'\b\d{10}\b', '',
                               texto)

    texto_anonimizado = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b', '',
                               texto_anonimizado)

    return texto_anonimizado

import requests
def processDfLimit(df, columna_texto, funcion_api, column_name):
    """
    Procesa un DataFrame aplicando una función de API a cada fila,
    respetando un límite de frecuencia de 10 solicitudes por minuto.
    Args:
        df (pd.DataFrame): El DataFrame a procesar.
        columna_texto (str): El nombre de la columna con el texto a procesar.
        funcion_api (callable): La función que realiza la llamada a la API.
    Returns:
        pd.DataFrame: El DataFrame con los resultados de la API.
    """
    resultados = []
    solicitudes_minuto = 0
    inicio_minuto = time.time()

    for texto in df[columna_texto]:
        # Added try-except block to handle ReadTimeout errors
        try:
            resultado = funcion_api(texto)
        except  requests.exceptions.ReadTimeout:
            print(f"ReadTimeout error for text: {texto[:50]}...")
            resultado = None
        resultados.append(resultado)
        solicitudes_minuto += 1

        if solicitudes_minuto >= 10:
            tiempo_transcurrido = time.time() - inicio_minuto
            if tiempo_transcurrido < 60:
                time.sleep(60 - tiempo_transcurrido)
            solicitudes_minuto = 0
            inicio_minuto = time.time()

    df[column_name] = resultados
    return df

def categorizedSentiment(text):
  import re
  text = text.lower()
  if re.findall('positivo', text):
    response = 'positivo'
  elif re.findall('negativo', text):
    response = 'negativo'
  else:
    response = 'neutro'
  return response

import re

def detectSpecialWords(text, textList):
  regularExp  = r'\b(?:'+'|'.join(textList)+ r')\b'
  detectedText = re.findall(regularExp, text, flags=re.IGNORECASE)
  if detectedText:
    return True
  else:
    return False

def extractVerbs(text):
    doc = nlp(text)
    verbs = [token.text for token in doc if token.pos_ == "VERB"]
    return '_'.join(verbs)

def extractNouns(text):
    doc = nlp(text)
    verbs = [token.text for token in doc if token.pos_ == "NOUN"]
    return '_'.join(verbs)

model = genai.GenerativeModel('models/gemma-3-27b-it')

texto = 'No me gustan los cacahuates'
model.generate_content(f"Analiza el sentimiento de este: {texto} solo dime si es positivo, negativo o neutro sin más contexto")

"""## General Libraries"""

import pandas as pd

listTextCars = ['autos', 'automóvil', 'auto', 'carro', 'camioneta',
            'cacharro']

"""## Facebook Categorized Text"""

facebookDf = pd.read_csv('/content/Facebook_raw.csv')

facebookDf.columns

facebookDf = processDfLimit(facebookDf, 'comment_text', getSummaryIA,
                            column_name='summary_text')

facebookDf['sentiment_value'] = facebookDf['comment_text'].apply(combinedSentimentAnalysis)

facebookDf['sentiment_text'] = facebookDf['sentiment_value'].apply(categorySentiment)

facebookDf['comment_text'] = facebookDf['comment_text'].apply(anonimizar_datos)

#facebookDf['sentiment_text'] = facebookDf['sentiment_text'].apply(categorizedSentiment)

facebookDf['car_related_text'] = facebookDf['comment_text'].apply(detectSpecialWords,
                                                                  textList=listTextCars)

facebookDf.head()

facebookDf.to_csv('Facebook_edited.csv')

"""## Google Maps Categorized Text"""

mapsDf = pd.read_csv('/content/Maps_raw.csv')

mapsGeneral= pd.read_csv('/content/Maps_edited.csv')

mapsDf = mapsDf[mapsDf['city'] != 'Mexico City']

# Commented out IPython magic to ensure Python compatibility.
# %timeit
mapsDf = mapsDf[['Input','name','Rating','num_review','author_title',
                 'author_rating', 'review_text','review_likes','review_link',
                 'city']]

mapsDf.columns

mapsDf.shape

mapsDf['summary_text'] = mapsDf['review_text'].apply(generate_summary)

mapsDf.head()

#mapsDf = processDfLimit(mapsDf, 'review_text', getSummaryIA,
#                            column_name='summary_text_2')

#%timeit
mapsDf['sentiment_value'] = mapsDf['review_text'].apply(combinedSentimentAnalysis)

mapsDf['sentiment_text'] = mapsDf['sentiment_value'].apply(categorySentiment)

mapsDf['review_text'] = mapsDf['review_text'].apply(anonimizar_datos)

mapsDf['car_related_text'] = mapsDf['review_text'].apply(detectSpecialWords,
                                                                  textList=listTextCars)

mapsDf = pd.concat([mapsDf, mapsGeneral])

mapsDf.shape

mapsDf.head()

mapsDf.to_csv('Maps_edited2.csv',
              index=False)

"""## Youtube and Places (Maps)"""

from googleapiclient.discovery import build
import pandas as pd

google_api_key = userdata.get('google_api_key')
youtube = build('youtube', 'v3', developerKey=google_api_key)

ids_videos = ['1hUuhcHJAA8', 'BRPujwCGQyw', 'uNuRIDdhT-4', 'sBkFoBQige8']

def get_comments(video_id):
  comments = []
  next_page_token = None

  while True:
    response = youtube.commentThreads().list(
        part='snippet',
        videoId=video_id,
        maxResults=100,
        pageToken=next_page_token
    ).execute()

    for item in response['items']:
      comment = item['snippet']['topLevelComment']['snippet']
      comments.append({
          'author': comment['authorDisplayName'],
          'text': comment['textDisplay'],
          'published_at': comment['publishedAt']
      })

    next_page_token = response.get('nextPageToken')
    if not next_page_token:
      break

  return comments


#comments = get_comments(video_id)

#df_comments = pd.DataFrame(comments)

df_list =[]
for id in ids_videos:
  comments = get_comments(id)
  df_comments = pd.DataFrame(comments)
  df_comments['video_id'] = 'https://www.youtube.com/watch?v=' + id
  df_list.append(df_comments)

youtubeDf = pd.concat(df_list)

youtubeDf.shape

youtubeDf.to_csv('youtubeDf.csv')

"""## Youtube Categorized Text"""

youtubeDf = pd.read_csv('/content/Youtube_raw.csv')

youtubeDf.columns

youtubeDf = processDfLimit(youtubeDf, 'text', getSummaryIA,
                            column_name='summary_text')

youtubeDf['sentiment_value'] = youtubeDf['text'].apply(combinedSentimentAnalysis)

youtubeDf['sentiment_text'] = youtubeDf['sentiment_value'].apply(categorySentiment)

youtubeDf['text'] = youtubeDf['text'].apply(anonimizar_datos)

youtubeDf['car_related_text'] = youtubeDf['text'].apply(detectSpecialWords,
                                                                  textList=listTextCars)

youtubeDf.head()

youtubeDf.to_csv('Youtube_edited.csv', index=False)

"""## Emotions model"""

#### Starting now to test the emotions model
from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax

model_path = "daveni/twitter-xlm-roberta-emotion-es"
tokenizer = AutoTokenizer.from_pretrained(model_path )
config = AutoConfig.from_pretrained(model_path )
emotions_model = AutoModelForSequenceClassification.from_pretrained(model_path)

testText = 'Odio esperar a la gente'

encoded_input = tokenizer(testText, return_tensors='pt')
output = emotions_model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)


ranking = np.argsort(scores)
ranking = ranking[::-1]
#for i in range(scores.shape[0]):
for i in np.sort(range(scores.shape[0])):
    l = config.id2label[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")

def detectEmotion(text):
  ### Get the pretrained model
  model_path = "daveni/twitter-xlm-roberta-emotion-es"
  tokenizer = AutoTokenizer.from_pretrained(model_path )
  config = AutoConfig.from_pretrained(model_path )
  emotions_model = AutoModelForSequenceClassification.from_pretrained(model_path)
  ### Starting the encoding
  text = str(text)
  encoded_input = tokenizer(text, return_tensors='pt')
  try:
    output = emotions_model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    emotions_score = np.sort(range(scores.shape[0]))
    emotions_score= emotions_score[0]
    l = config.id2label[ranking[emotions_score]]
    s = scores[ranking[emotions_score]]
    return l, np.round(float(s), 4)
  except:
    return None, None
  #output = emotions_model(**encoded_input)
  #return emotions_score

detectEmotion(testText)

import pandas as pd
mapsDfedited =  pd.read_csv('/content/Maps_edited.csv')

#mapsDfedited['emotion_text'] = mapsDfedited['review_text'].apply(lambda x: detectEmotion(x)[0])
#mapsDfedited['emotion_text_score'] = mapsDfEdited['review_text'].apply(lambda x: detectEmotion(x)[1])
mapsDfedited[['emotion_text', 'emotion_text_score']] = mapsDfedited['review_text'].apply(lambda x: pd.Series(detectEmotion(x)))

mapsDfedited.head()

mapsDfedited.to_csv('Maps_edited.csv')

from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax
# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)
model_path = "daveni/twitter-xlm-roberta-emotion-es"
tokenizer = AutoTokenizer.from_pretrained(model_path )
config = AutoConfig.from_pretrained(model_path )
# PT
model = AutoModelForSequenceClassification.from_pretrained(model_path )
text = "Se ha quedao bonito día para publicar vídeo, ¿no? Hoy del tema más diferente que hemos tocado en el canal."
text = preprocess(text)
print(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
# Print labels and scores
ranking = np.argsort(scores)
ranking = ranking[::-1]
for i in range(scores.shape[0]):
    l = config.id2label[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")